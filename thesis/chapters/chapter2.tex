\chapter{Monte Carlo Methods Applied to the Ising Model}

A short review of the Monte Carlo (MC) methods used to solve the Ising Model in the current paradigm is presented. There will be a focus on the famous Metropolis Method and the Wang-Landau sampling. 

\section{Metropolis Method}

The classic Metropolis method, introduced in 1953 by Metropolis et al., belongs to the Markov chain Monte Carlo (MCMC) class of algorithms. These algorithms exploit the fact that if we construct a Markov chain that has a specific equilibrium distribution one can obtain samples recording the states generated by the Markov chain. 

In a brief fashion, a Markov chain is a stochastic model that describes a sequence of possible events, in this case microstates,in which the probability of transiting to another state depends on the current state. This way the probability of the next state, $S_j$, given the current state, $S_i$, can be written as 
\begin{equation}
	P(S_j, t) = \sum_i W(S_i \rightarrow S_j) P(S_i, t),
\end{equation}
where $W(S_i \rightarrow S_j) \equiv W_{ij}$ is the transition probability to move from the state $i$ to $j$. We require that 
\begin{equation}
	W_{ij}  \geq 0 \quad \quad \quad \quad \quad \sum_j W_{ij}=1.
\end{equation}
The master equation considers the change of the probability of the next state with time, $t$,
\begin{equation}
	\frac{dP(S_j, t)}{dt} = \sum_i \left[ W_{ij}P(S_i, t) - W_{ji} P(S_j,t)  \right].
\end{equation}
In the equilibrium regime, the master equation has to equal $0$, and we get the detailed balance condition for the equilibrium probability $P_{eq}(S_j)$,
\begin{equation}
	W_{ji}P_{eq}(S_j) = W_{ij}P_{eq}(S_i).
\end{equation}

The object of Metropolis Sampling is to generate canonical configurations with an equilibrium probability
\begin{equation} \label{eq:met_prob_eq}
	P_{eq}(E_i) = \frac{\exp(-\beta E_i)}{Z}.
\end{equation}
Here $Z$ is the partition function, however this is usually not know before hand. When considering a Markovian process we generate each new configuration from the preceding one avoiding this problem. As a result the difference of energy between the two states is needed, $\Delta E = E_i - E_j$ and the transition probability of given as

\begin{equation}
	W_{ij} = \twopartdef { \tau_0^{-1} \exp(-\beta \Delta E) } {\Delta E \geq 0} {\tau_0^{-1}} {\Delta E < 0}
\end{equation}
where $\tau_0^{-1}$ is the time required to attempt a spin-flip.  We often set this time unit to one.

This way the Metropolis method applied to the Ising Model, with a fixed external magnetic field and a fixed temperature, goes as follows:

\begin{enumerate}
\item Choose an initial state;
\item Choose a spin $i$ and perform a spin-flip;                                                                                                                                       
\item Calculate the energy change from that spin-flip, $\Delta E$;
\item Accept the flip with a probability $\min \left( 1, \exp(-\beta \Delta E) \right)$; 
\item When the system reaches equilibrium, measure any thermodynamic quantity needed;                                                                                                        
\item Go back to (2) and repeat until there is enough samples of the thermodynamic variables.                                                                                                                    
\end{enumerate}
Note that we accept the spin flip if a given uniformly random number $r, r \in [0,1]$, is less or equal to the acceptance criteria. Typically in Monte Carlo simulations we define the MC time as the amount of trial flips equal to the number of spins in our lattice, N.

\subsection{Success and Limitations}

The Metropolis sampling proposed by Metropolis et al., can be successfully applied to an array of models, ranging from widely studied quantum ensembles and gases simulations to state-of-the-art protein and peptide simulations and to machine learning and neural networks. The following paragraphs will be directed to magnetic systems, like the Ising model, but they can be extrapolated to others physical systems. 

When getting thermodynamic variables we have to let the simulation reach the equilibrium stage, where the probability distribution takes the form of equation \ref{eq:met_prob_eq}. Then we take a measurement each MC time, resulting in $M$ total values for that variable. At the end the average if that variable, $A$, is taken $\langle A \rangle = \frac{1}{M} \sum_i A_i$. 
For large systems the time taken to reach equilibrium stage is often very long thus making the simulation time consuming. 
This is worsened by the fact that to study how some thermodynamic variable $A$ changes over a wide range of temperatures or applied fields intensities, we need to run multiple Metropolis simulations for each temperature and field intensity values, making this process very time-consuming.

Lastly there is another shortcoming known as critical slowing down. In short, for computations where the temperature is near the critical temperature, $T_C$, the sampling slows down, meaning that it is more time-consuming for the computations to reach the equilibrium stage, thus slowing down the overall simulation.

\section{Wang-Landau Sampling}

Since its introduction the Metropolis sampling was the go-to method to study phase transitions and critical phenomena in condensed matter physics and statistical mechanics. In the final decades of the 20th century scientists were committed to develop new methods that could overcome the shortcomings of the Metropolis Sampling. Various methods were proposed such as the cluster flip algorithms, where Swendsen and Wang where pioneers, and the multicanonical ensemble method. The first solved the critical slowing down present in the Metropolis and the second could sample rough energy landscapes with ease. 

In 2001, Fugao Wang and David P. Landau proposed a new Monte Carlo method, now called the Wang-Landau method or sampling. The goal of this method diverges from the goal of previous methods. Instead of generating configurations with a canonical probability, this method tries to estimate the canonical partition function
\begin{equation}
	Z = \sum_E g(E) \exp(-\beta E)
\end{equation}
through the estimation of the density of states $g(E)$ from a flat histogram in the phase space.

The main difficulty of a simple random walk in the energy space is that the walker would spend most of its time in the highest probable states thus making the random walk ineffective. The idea of the Wang-Landau sampling is to do a random walk,Metropolis like, and accepting the new states with a probability proportional to the inverse to their DoS $\frac{1}{g(E)}$. Doing this we obtain a flat histogram meaning that each macrostate has the same probability of being visited.  
Since $g(E)$ is unknown \textit{Ã  priori}, in each step of the random walk, $g(E)$ is also being constructed.

The algorithm was proposed to compute the DoS but, it can also estimate the JDoS by performing the random walk in the phase space composed by energy, $E$, and the second order parameter, in our case, the magnetization of the system, $M$. This comes with a downside, since the JDoS has 1000x more information than the DoS it takes 1000x more time to compute.

\subsection{Algorithm}

First we start with an arbitrary configuration of spins and a guess for the density of states. Usually,this  guess is $g(E, M)=1$. Choosing a random site in our lattice, we perform a trial flip and compute the energy-magnetization pair before the trial, $(E_i, M_i)$, and after, $(E_j,M_j)$. The new configuration is accepted with a probability
\begin{equation}
	P((E_i, M_i) \rightarrow (E_j, M_j)) = \min\left(1, \frac{g(E_i, M_i)}{g(E_j, M_j)}\right).
\end{equation}
Whether the configuration is accepted or rejected, we have to update the histogram and refine the DoS estimation. This way, being the system in the state $(E, M)$,
\begin{equation*}
	H(E, M) = H(E,M)+1,
\end{equation*}
and we multiply the current value of the DoS by a modification factor, $f > 1$,
\begin{equation*}
	g(E,M)=fg(E,M).
\end{equation*}
A reasonable choice for the initial value of the modification factor if $f_0 = e$. If $f_0$ is too small, the simulation will take a very long time to reach all of the possible macrostates, $(E,M)$. If it is too large, then we will have large statistical errors.  
This process is repeated until the histogram is considered "flat", all of the possible energies have been visited the same number of times. As it is impossible to obtain a $100\%$ flat histogram, we define a rule for flatness as $\min(H(E, M)) > \langle H(E, M) \rangle \times p$; $p$ is chosen according to the size of the problem. For small cases, such as the two dimensional Ising model, $p$ can be set as high as $0.95$, but for larger systems the flatness condition may never be satisfied if $p$ is near unity.  
Once a flat histogram is reached, we set $H(E, M)=0$, keep the estimation of the DoS and reduce the modification factor, $\sqrt{f_{i}} \rightarrow f_{i+1}$ and continue the random walk. We stop the simulation if $f<f_{final}$, where $f_{final}$ is a number very close to one (often $f_{final} \sim 1+1E-8$). 

At the end, the method gives us the relative density of states. To determine the normalized DoS we can use the fact that 
\begin{equation}
	\sum_{E,M} g(E,M) = 2^N.
\end{equation}

\subsection{Success and Limitations}

The modification factor controls the accuracy and the steps taken to reach a flat histogram in our computations. As it approaches unity, the number of iterations goes to infinity. This way, at the beginning of the simulation, when the modification factor is large, the estimation of the DoS is quite bad, since there we are taking fewer samples with a high weight. At the later stages, the modification factor is lower therefore we have more samples of each macrostate with a lower overall weight. 
This way, the initial stages of the simulation are characterized by the accumulation of low precision statistics and the later stages by the refining of the firsts samples. 

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6, height=5cm]{wl_mean_error_f.pdf}
	\caption{TODO: MORE P VALUES!!!! Mean absolute error of the JDoS for the Ising model computed by the Wang-Landau sampling for a L4 SS lattice plotted against $f_{final}-1$. The flatness condition used was 90\%.}
	\label{error_abs_wl}
\end{figure}
Due to the wrong samples at the initial stages of the computations, the final estimation of the JDoS will never converge to the exact solution. In the figure \ref{error_abs_wl} we can see that the absolute value of the mean error never converges to zero, thus the method has an intrinsic source of error. 








